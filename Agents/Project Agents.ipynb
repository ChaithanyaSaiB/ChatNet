{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Written Agent without binding to tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages in determine mode function:\n",
      "human: Tell me a joke\n",
      "-------------------\n",
      "Response generated: llm\n",
      "Response: Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n",
      "Messages in determine mode function:\n",
      "human: Tell me a joke\n",
      "ai: Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n",
      "human: What was the last thing we talked about?\n",
      "-------------------\n",
      "Response generated: llm\n",
      "Response: We talked about a joke! I shared a bicycle-themed joke with you, and it was a bit of a \"tire\"-d pun.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from typing import Annotated, Literal, Union\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "# Create the LLM model and TavilySearchResults\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    mode: Literal[\"llm\", \"search\", \"NA\"]\n",
    "\n",
    "# Define the function to determine the mode\n",
    "def determine_mode(state: State) -> State:\n",
    "    print(\"Messages in determine mode function:\")\n",
    "    for msg in state['messages']:\n",
    "        print(f\"{msg.type}: {msg.content}\")\n",
    "    print(\"-------------------\")\n",
    "    conversation_history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in state['messages']])\n",
    "    latest_query = state[\"messages\"][-1].content\n",
    "    \n",
    "    prompt = f\"\"\"Based on the conversation history and the latest user query below, determine if the query can be answered by LLM using the context from the conversation, or if it really needs an external web search to gather new information to answer the query. Try to not use web search unless necessary.\n",
    "    \n",
    "    Respond with ONLY \"llm\" or \"search\" without any additional text or explanation.\n",
    "    \n",
    "    Conversation history:\n",
    "    {conversation_history}\n",
    "    \n",
    "    Latest query: {latest_query}\n",
    "\n",
    "    Response (ONLY \"llm\" or \"search\"):\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    print(\"Response generated:\", response.content.strip().lower())\n",
    "\n",
    "    if \"search\" in response.content.strip().lower():\n",
    "        return {\"mode\": \"search\"}\n",
    "    return {\"mode\": \"llm\"}\n",
    "\n",
    "def llm_response(state: State) -> State:\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response], \"mode\": \"NA\"}\n",
    "\n",
    "def search_response(state: State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_result = tavily_search.invoke(query)\n",
    "\n",
    "    prompt = f\"You are an AI assistant. Use the following search results to answer the user's question:\\n{search_result}\\n\\nUser query: {query}\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response], \"mode\": \"NA\"}\n",
    "\n",
    "# Create the graph\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"determine_mode\", determine_mode)\n",
    "graph.add_node(\"llm_response\", llm_response)\n",
    "graph.add_node(\"search_response\", search_response)\n",
    "\n",
    "# Add edges\n",
    "graph.set_entry_point(\"determine_mode\")\n",
    "graph.add_conditional_edges(\n",
    "    \"determine_mode\",\n",
    "    lambda x: x[\"mode\"],\n",
    "    {\n",
    "        \"llm\": \"llm_response\",\n",
    "        \"search\": \"search_response\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"llm_response\", END)\n",
    "graph.add_edge(\"search_response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "chain = graph.compile()\n",
    "\n",
    "# Initialize an empty conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Function to add a new message and get a response\n",
    "def get_response(user_input):\n",
    "    # Add the new user message to the conversation history\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # Invoke the chain with the full conversation history\n",
    "    result = chain.invoke({\"messages\": conversation_history})\n",
    "    \n",
    "    # Add the AI's response to the conversation history\n",
    "    conversation_history.append(result['messages'][-1])\n",
    "    \n",
    "    print(\"Response:\", result['messages'][-1].content)\n",
    "\n",
    "# Example usage\n",
    "get_response(\"Tell me a joke\")\n",
    "get_response(\"What was the last thing we talked about?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages in determine mode function:\n",
      "human: Tell me a joke\n",
      "ai: Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n",
      "human: What was the last thing we talked about?\n",
      "ai: We talked about a joke! I shared a joke with you about a bicycle being two-tired.\n",
      "human: explain it to me as if I am 5 year old\n",
      "-------------------\n",
      "Response generated: llm\n",
      "Response: OH BOY!\n",
      "\n",
      "So, you know how bicycles have wheels, right? Like, round things that make the bike move?\n",
      "\n",
      "Well, the joke is saying that the bicycle is \"two-tired\". Get it? \"Two-tired\" instead of \"too tired\"!\n",
      "\n",
      "It's like saying the bike is tired, but it's also making a funny pun on the word \"tired\" because it has TWO TIRES! \n",
      "\n",
      "So, the joke is saying that the bike can't stand up by itself because it's \"two-tired\"... it's a play on words!\n"
     ]
    }
   ],
   "source": [
    "get_response(\"explain it to me as if I am 5 year old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages in determine mode function:\n",
      "human: Tell me a joke\n",
      "ai: Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n",
      "human: What was the last thing we talked about?\n",
      "ai: We talked about a joke! I shared a joke with you about a bicycle being two-tired.\n",
      "human: explain it to me as if I am 5 year old\n",
      "ai: OH BOY!\n",
      "\n",
      "So, you know how bicycles have wheels, right? Like, round things that make the bike move?\n",
      "\n",
      "Well, the joke is saying that the bicycle is \"two-tired\". Get it? \"Two-tired\" instead of \"too tired\"!\n",
      "\n",
      "It's like saying the bike is tired, but it's also making a funny pun on the word \"tired\" because it has TWO TIRES! \n",
      "\n",
      "So, the joke is saying that the bike can't stand up by itself because it's \"two-tired\"... it's a play on words!\n",
      "human: thats so cool. how can i start my own bank?\n",
      "-------------------\n",
      "Response generated: search\n",
      "Response: Starting your own bank! That's quite an ambitious goal! According to the search results, it's not a trivial task, but I'll break it down for you.\n",
      "\n",
      "**Option 1: Traditional Banking**\n",
      "To start a bank in the United States, you'll need to have a substantial amount of capital, upwards of $20,000,000, and assemble an expert team of professionals. This route requires a significant investment and a deep understanding of the banking industry.\n",
      "\n",
      "**Option 2: Alternative Banking**\n",
      "However, there's an alternative approach. You can \"become your own bank\" by leveraging the cash value of a properly-structured whole life insurance policy. This method allows you to borrow against the policy's value, essentially acting as your own bank. This route requires a thorough understanding of insurance policies and financial planning.\n",
      "\n",
      "Before you begin, consider the following general steps:\n",
      "\n",
      "1. **Determine a need**: Identify the purpose of your bank and the services you want to offer.\n",
      "2. **Assemble a team**: Gather experts in finance, law, and banking to help you navigate the process.\n",
      "3. **Secure starting capital**: Ensure you have the necessary funds to launch and operate your bank.\n",
      "4. **Create a business plan**: Develop a comprehensive plan outlining your bank's operations, financial projections, and risk management strategies.\n",
      "5. **Hire a legal team**: Consult with lawyers to ensure compliance with banking regulations and laws.\n",
      "\n",
      "Remember, starting a bank is a complex and challenging endeavor. It's essential to carefully evaluate your options, assess your resources, and consider seeking professional guidance before embarking on this venture.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"thats so cool. how can i start my own bank?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres integrated Langgraph Agent with partial Langsmith tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages in determine mode function:\n",
      "human: give one word reply for this: hi\n",
      "ai: Hello\n",
      "human: give one word reply for this: hi\n",
      "ai: Hey\n",
      "human: give one word reply for this: hi\n",
      "ai: Hi\n",
      "human: give one word reply for this: hi\n",
      "-------------------\n",
      "Response generated: llm\n",
      "Response: Hey\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from typing import Annotated, Literal, Union\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from psycopg_pool import AsyncConnectionPool\n",
    "from psycopg.rows import dict_row\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.callbacks.manager import CallbackManager, AsyncCallbackManager\n",
    "from langsmith import trace\n",
    "\n",
    "# Set up LangSmith\n",
    "client = Client(api_url=os.environ[\"LANGSMITH_ENDPOINT\"], api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "tracer = LangChainTracer(project_name=os.environ[\"LANGSMITH_PROJECT\"])\n",
    "callback_manager = AsyncCallbackManager([tracer])\n",
    "\n",
    "# Modify your LLM creation to include callbacks\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", callbacks=[tracer])\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    mode: Literal[\"llm\", \"search\", \"NA\"]\n",
    "\n",
    "# Define the function to determine the mode\n",
    "def determine_mode(state: State) -> State:\n",
    "    with trace(\"determine_mode\") as run:\n",
    "        print(\"Messages in determine mode function:\")\n",
    "        for msg in state['messages']:\n",
    "            print(f\"{msg.type}: {msg.content}\")\n",
    "        print(\"-------------------\")\n",
    "        conversation_history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in state['messages']])\n",
    "        latest_query = state[\"messages\"][-1].content\n",
    "        \n",
    "        prompt = f\"\"\"Based on the conversation history and the latest user query below, determine if the query can be answered by LLM using the context from the conversation, or if it really needs an external web search to gather new information to answer the query. Try to not use web search unless necessary.\n",
    "        \n",
    "        Respond with ONLY \"llm\" or \"search\" without any additional text or explanation.\n",
    "        \n",
    "        Conversation history:\n",
    "        {conversation_history}\n",
    "        \n",
    "        Latest query: {latest_query}\n",
    "\n",
    "        Response (ONLY \"llm\" or \"search\"):\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        print(\"Response generated:\", response.content.strip().lower())\n",
    "\n",
    "        if \"search\" in response.content.strip().lower():\n",
    "            run.end(outputs={\"mode\": \"search\"})\n",
    "            return {\"mode\": \"search\"}\n",
    "        run.end(outputs={\"mode\": \"llm\"})\n",
    "        return {\"mode\": \"llm\"}\n",
    "\n",
    "def llm_response(state: State) -> State:\n",
    "    with trace(\"llm_response\") as run:\n",
    "        messages = state[\"messages\"]\n",
    "        response = llm.invoke(messages)\n",
    "        run.end(outputs={\"messages\": [response], \"mode\": \"NA\"})\n",
    "        return {\"messages\": [response], \"mode\": \"NA\"}\n",
    "\n",
    "def search_response(state: State) -> State:\n",
    "    with trace(\"search_response\") as run:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        search_result = tavily_search.invoke(query)\n",
    "\n",
    "        prompt = f\"You are an AI assistant. Use the following search results to answer the user's question:\\n{search_result}\\n\\nUser query: {query}\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        run.end(outputs={\"messages\": [response], \"mode\": \"NA\"})\n",
    "        return {\"messages\": [response], \"mode\": \"NA\"}\n",
    "\n",
    "def create_agent(checkpointer):\n",
    "    # Create the graph\n",
    "    graph = StateGraph(State)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"determine_mode\", determine_mode)\n",
    "    graph.add_node(\"llm_response\", llm_response)\n",
    "    graph.add_node(\"search_response\", search_response)\n",
    "\n",
    "    # Add edges\n",
    "    graph.set_entry_point(\"determine_mode\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"determine_mode\",\n",
    "        lambda x: x[\"mode\"],\n",
    "        {\n",
    "            \"llm\": \"llm_response\",\n",
    "            \"search\": \"search_response\"\n",
    "        }\n",
    "    )\n",
    "    graph.add_edge(\"llm_response\", END)\n",
    "    graph.add_edge(\"search_response\", END)\n",
    "\n",
    "    # Compile the graph\n",
    "    chain = graph.compile(checkpointer=checkpointer)\n",
    "    return chain\n",
    "\n",
    "async def setup_pool():\n",
    "    return AsyncConnectionPool(\n",
    "        conninfo=os.getenv(\"DATABASE_URL\"),\n",
    "        max_size=20,\n",
    "        kwargs={\n",
    "            \"autocommit\": True,\n",
    "            \"prepare_threshold\": 0,\n",
    "            \"row_factory\": dict_row,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to add a new message and get a response\n",
    "async def get_response(pool, user_input):\n",
    "    async with pool.connection() as conn:\n",
    "        try:\n",
    "            # Initialize persistent chat memory\n",
    "            memory = AsyncPostgresSaver(conn)\n",
    "\n",
    "            # IMPORTANT: You need to call .setup() the first time you're using your memory\n",
    "            await memory.setup()\n",
    "\n",
    "            # Create a LangGraph agent\n",
    "            langgraph_agent = create_agent(checkpointer=memory)\n",
    "\n",
    "            with trace(\"LangGraph Agent\") as parent_run:\n",
    "                response = await langgraph_agent.ainvoke(\n",
    "                    {\"messages\": [HumanMessage(content=user_input)]},\n",
    "                    {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "                )\n",
    "            \n",
    "            # Process the chunks from the agent\n",
    "            return response['messages'][-1].content\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "async with await setup_pool() as pool:\n",
    "    response = await get_response(pool, \"give one word reply for this: hi\")\n",
    "    print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment for langsmith full tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call model's response: content='' additional_kwargs={'tool_calls': [{'id': 'call_jdjk', 'function': {'arguments': '{\"query\":\"current prime minister of Israel\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 6777, 'total_tokens': 6821, 'completion_time': 0.127876143, 'prompt_time': 0.342764971, 'queue_time': 0.04144218500000002, 'total_time': 0.470641114}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-527c77fb-ce3d-4eec-b003-031638c27ec4-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current prime minister of Israel'}, 'id': 'call_jdjk', 'type': 'tool_call'}] usage_metadata={'input_tokens': 6777, 'output_tokens': 44, 'total_tokens': 6821}\n",
      "Call model's response: content='The Prime Minister of Israel is Benjamin Netanyahu.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 7013, 'total_tokens': 7023, 'completion_time': 0.030003004, 'prompt_time': 0.349645143, 'queue_time': 0.037797633, 'total_time': 0.379648147}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None} id='run-bc2de3ea-f311-4314-8cfc-09d2d8252cf9-0' usage_metadata={'input_tokens': 7013, 'output_tokens': 10, 'total_tokens': 7023}\n",
      "Response: The Prime Minister of Israel is Benjamin Netanyahu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import operator\n",
    "from typing import Annotated, Literal, Union\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from psycopg_pool import AsyncConnectionPool\n",
    "from psycopg.rows import dict_row\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "from psycopg_pool import ConnectionPool\n",
    "\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.callbacks.manager import CallbackManager, AsyncCallbackManager\n",
    "from langsmith import trace\n",
    "import cProfile\n",
    "\n",
    "\n",
    "# Set up LangSmith\n",
    "client = Client(api_url=os.environ[\"LANGSMITH_ENDPOINT\"], api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "tracer = LangChainTracer(project_name=os.environ[\"LANGSMITH_PROJECT\"])\n",
    "callback_manager = CallbackManager([tracer])\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "tools = [tavily_search]\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Modify your LLM creation to include callbacks\n",
    "model = ChatGroq(model=\"llama3-70b-8192\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    mode: Literal[\"llm\", \"search\", \"NA\"]\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    system_message = SystemMessage(content=\"You are an AI assistant with access to tools. When you need more information to answer a question, use the appropriate tool. Always use tools for current events or data that might change.\")\n",
    "    messages = [system_message] + messages\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    print(\"Call model's response:\", response)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def create_agent(checkpointer):\n",
    "    # Create the graph\n",
    "    graph = StateGraph(MessagesState)\n",
    "\n",
    "    graph.add_node(\"agent\", call_model)\n",
    "    graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "    graph.add_edge(\"__start__\", \"agent\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "    )\n",
    "    graph.add_edge(\"tools\", 'agent')\n",
    "\n",
    "\n",
    "    # Compile the graph\n",
    "    chain = graph.compile(checkpointer=checkpointer)\n",
    "    return chain\n",
    "\n",
    "def setup_pool():\n",
    "    return ConnectionPool(\n",
    "        conninfo=os.getenv(\"DATABASE_URL\"),\n",
    "        max_size=20,\n",
    "        kwargs={\n",
    "            \"autocommit\": True,\n",
    "            \"prepare_threshold\": 0,\n",
    "            \"row_factory\": dict_row,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# # Function to add a new message and get a response\n",
    "# async def get_response(pool, user_input):\n",
    "#     async with pool.connection() as conn:\n",
    "#         try:\n",
    "#             # Initialize persistent chat memory\n",
    "#             memory = AsyncPostgresSaver(conn)\n",
    "\n",
    "#             # IMPORTANT: You need to call .setup() the first time you're using your memory\n",
    "#             await memory.setup()\n",
    "\n",
    "#             # Create a LangGraph agent\n",
    "#             langgraph_agent = create_agent(checkpointer=memory).with_config(callback_manager=callback_manager)\n",
    "\n",
    "#             response = await langgraph_agent.ainvoke(\n",
    "#                 {\"messages\": [HumanMessage(content=user_input)]},\n",
    "#                 {\"configurable\": {\"thread_id\": \"7\"},\n",
    "#                  \"callbacks\": callback_manager}\n",
    "#             )\n",
    "\n",
    "#             # Process the chunks from the agent\n",
    "#             return response['messages'][-1].content\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred: {e}\")\n",
    "#             raise\n",
    "\n",
    "# async with await setup_pool() as pool:\n",
    "#     response = await get_response(pool, \"who is the president of the maldives?\")\n",
    "#     print(\"Response:\", response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# langgraph_agent = create_agent()\n",
    "\n",
    "# response = langgraph_agent.invoke(\n",
    "#     {\"messages\": [HumanMessage(content=\"who is the president of the united states?\")]},\n",
    "#     {\"configurable\": {\"thread_id\": \"7\"}}\n",
    "# )\n",
    "\n",
    "# # Process the chunks from the agent\n",
    "# print(response['messages'][-1].content)\n",
    "\n",
    "\n",
    "\n",
    "def get_response(memory, user_input):\n",
    "    try:\n",
    "        langgraph_agent = create_agent(checkpointer=memory)\n",
    "        response = langgraph_agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            {\"configurable\": {\"thread_id\": \"7\"}}\n",
    "        )\n",
    "        return response['messages'][-1].content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "with setup_pool() as pool:\n",
    "    with pool.connection() as conn:\n",
    "        memory = PostgresSaver(conn)\n",
    "        memory.setup()  # Synchronous setup\n",
    "        response = get_response(memory, \"who is the PM of israel?\")\n",
    "        print(\"Response:\", response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatnetv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
